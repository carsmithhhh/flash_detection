{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2e87d9-2550-4dba-9a55-9aac7fa444ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sdf/home/c/carsmith/flash_reconstruction/flash_detection/notebooks'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fe665a-6dce-4c64-b6a9-5baac24602f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "from data_utils import *\n",
    "from waveforms.make_waveform import BatchedLightSimulation\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Subset\n",
    "import importlib\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08cd933d-4a1c-4633-9899-24a3cc81ca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split sizes: (63000, 7000, 0)\n"
     ]
    }
   ],
   "source": [
    "# Loading in data\n",
    "load_wfs = np.load('/sdf/home/c/carsmith/sdf_data/flash_detection_data/flash_files/delta_t_stratsamples.npy', allow_pickle=True) # array of shape (2519, 5, 250, 250)\n",
    "dataset = WaveformDataset(load_wfs.item())\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "# Splitting data\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.0\n",
    "total_size = len(dataset)\n",
    "val_size = int(total_size * val_ratio)\n",
    "test_size = int(total_size * test_ratio)\n",
    "train_size = total_size - val_size - test_size\n",
    "print(f\"split sizes: {train_size, val_size, test_size}\")\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=g)\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for WaveformDataset.\n",
    "    Each item in batch is a tuple: (waveform, arrival_time).\n",
    "    Returns:\n",
    "        waveforms: Tensor of shape (batch_size, waveform_length)\n",
    "        arrival_times: Tensor of shape (batch_size,) or (batch_size, 1)\n",
    "        hit_times: Tensor of shape (?) with a list of hit times per sample\n",
    "    \"\"\"\n",
    "    waveforms, arrival_times, hit_times, photon_bins, photon_list = zip(*batch)\n",
    "    waveforms = torch.stack(waveforms, dim=0) # [B, L]\n",
    "\n",
    "    # Normalizing waveforms\n",
    "    waveforms = (waveforms - waveforms.mean(dim=1, keepdim=True)) / (waveforms.std(dim=1, keepdim=True) + 1e-8)\n",
    "    waveforms = waveforms.unsqueeze(1)  # add channel dimension [B,1,L]\n",
    "\n",
    "    # for binary classification\n",
    "    arrival_times = torch.stack(arrival_times, dim=0)\n",
    "    arrival_times = arrival_times.unsqueeze(1) # adding channel dimension\n",
    "    photon_bins = torch.stack(photon_bins, dim=0)\n",
    "    photon_bins = photon_bins.unsqueeze(1)\n",
    "\n",
    "    # Normalize entire tensor\n",
    "    # max_count = photon_bins.max().item() if photon_bins.numel() > 0 else 1\n",
    "    # photon_bins = photon_bins / max_count\n",
    "\n",
    "    # for regression, just use hit times\n",
    "    hit_times = [item[2] for item in batch]\n",
    "    hit_times = torch.tensor(hit_times)\n",
    "    photon_list = [item[4] for item in batch]\n",
    "    photon_list = torch.tensor(photon_list)\n",
    "    # max_photons = photon_list.max().item() if photon_list.numel() > 0 else 1\n",
    "    # photon_list_norm = photon_list / max_photons\n",
    "    \n",
    "    return waveforms, arrival_times, hit_times, photon_bins, photon_list\n",
    "    \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    generator=g,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9c87c6-780d-4031-a995-04f7db0a390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lscratch/carsmith/tmp/ipykernel_980081/236944743.py:48: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  hit_times = torch.tensor(hit_times)\n"
     ]
    }
   ],
   "source": [
    "waveforms, arrival_times, hit_times, photon_bins, photon_list = next(iter(train_loader))\n",
    "print(hit_times[3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae399200-ddab-4738-97d4-0145c7348da8",
   "metadata": {},
   "source": [
    "### Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af83566-9cca-4355-a4c4-fd6b03325dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit_times: torch.Size([25, 2])\n",
      "torch.Size([25, 1, 8000])\n",
      "torch.Size([25, 1, 8000])\n",
      "torch.Size([25, 1, 8000])\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "waveforms, arrival_times, hit_times, photon_bins, photon_list = next(iter(train_loader))\n",
    "print(f\"hit_times: {hit_times.shape}\")\n",
    "print(waveforms.shape)\n",
    "print(arrival_times.shape)\n",
    "print(photon_bins.shape)\n",
    "\n",
    "waveform_id = 4\n",
    "wf = waveforms[waveform_id]  # [1, L]\n",
    "ticks = torch.arange(wf.shape[-1])  # [0, 1, ..., L-1]\n",
    "wf = wf.squeeze(0)  # [L]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2, 1, figsize=(12, 6), sharex=True,\n",
    "    gridspec_kw={'height_ratios': [3, 1]}\n",
    ")\n",
    "\n",
    "# --- Top panel: waveform ---\n",
    "ax1.plot(ticks, wf, alpha=1, label='Waveform')\n",
    "\n",
    "# Mark true hits\n",
    "for j, t in enumerate(hit_times[waveform_id]):\n",
    "    if t >= 0:\n",
    "        ax1.axvline(\n",
    "            x=t.item(), \n",
    "            color='r', \n",
    "            linestyle='--', \n",
    "            linewidth=1, \n",
    "            label='True arrival' if j == 0 else \"\"\n",
    "        )\n",
    "\n",
    "ax1.set_title(f\"Waveform {waveform_id}\")\n",
    "ax1.set_ylabel(\"ADC\")\n",
    "ax1.legend()\n",
    "\n",
    "# --- Bottom panel: photon counts ---\n",
    "photon_counts = photon_bins[waveform_id].cpu().numpy()\n",
    "print(photon_counts[0].sum())\n",
    "ax2.bar(ticks, photon_counts[0], width=20.0, color='b', alpha=0.6)\n",
    "ax2.set_ylabel(\"Photon Count\")\n",
    "ax2.set_xlabel(\"Time Tick (1 tick = 0.001 μs)\")\n",
    "ax2.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1e545-8c27-46fa-af7b-451858ad51da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcarsmith\u001b[0m (\u001b[33mcarsmith-stanford-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sdf/home/c/carsmith/flash_reconstruction/flash_detection/notebooks/wandb/run-20250909_141528-35nudrpl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/carsmith-stanford-university/unet_training_grid/runs/35nudrpl' target=\"_blank\">new_200k_lambda_001_poiss_log</a></strong> to <a href='https://wandb.ai/carsmith-stanford-university/unet_training_grid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/carsmith-stanford-university/unet_training_grid' target=\"_blank\">https://wandb.ai/carsmith-stanford-university/unet_training_grid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/carsmith-stanford-university/unet_training_grid/runs/35nudrpl' target=\"_blank\">https://wandb.ai/carsmith-stanford-university/unet_training_grid/runs/35nudrpl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   4%|█▎                            | 314/7200 [01:15<21:25,  5.36it/s, train_loss=0.344, train_acc=0.973, train_pure=0.0363]"
     ]
    }
   ],
   "source": [
    "# Doing the training yuhh\n",
    "from model import *\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "epochs = 50\n",
    "device = 'cuda'\n",
    "\n",
    "# try tracking with wandb\n",
    "logger = wandb.init(\n",
    "    project=\"unet_training_grid\",\n",
    "    name=\"new_200k_lambda_001_poiss_log\",\n",
    "    config={\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": 1e-4,\n",
    "    }\n",
    ")\n",
    "\n",
    "model = UNet1D()\n",
    "model.to(device)\n",
    "wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss() # combines sigmoid + loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "mode = 'mined_bce'\n",
    "results = train_model_2(model, train_loader, val_loader, scheduler, optimizer, device, epochs, mode, logger)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "}, f\"/sdf/home/c/carsmith/sdf_data/flash_detection_data/delay_200ks_ckpts/unet_28_50epochs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb42c57-ea60-41a5-bd2e-6f9eea5733e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "train_progress = tqdm(val_loader, leave=False, position=0)\n",
    "for i, (data, target, hit_times, photon_target, photon_list) in enumerate(train_progress):\n",
    "    data, target, hit_times, photon_target = data.to(device), target.to(device), hit_times.to(device), photon_target.to(device) # model output is [25, 992] but target is [25, 1000] due to mismatch in downsampling/upsampling shapes\n",
    "    class_output, reg_output = model(data)\n",
    "    print(class_output.shape)\n",
    "    break\n",
    "    \n",
    "rmse = regression_rmse(hit_times, photon_target, reg_output, class_output, device)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8a7c6-49b6-4fe2-9a5e-b6200d19db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results['train_pure'] and results['train_acc'] on subfigures next to each other\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "# Plot train_pure\n",
    "ax1.plot(results['train_pure'], linewidth=2)\n",
    "ax1.set_title(\"Training Purity\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Purity\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot train_acc\n",
    "ax2.plot(results['train_acc'], color=\"orange\", linewidth=2)\n",
    "ax2.set_title(\"Training Accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc7f18-bb65-482a-89aa-e345e1f34434",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_epochs = np.arange(len(results['eval_loss'])) * 5 + 5\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results['train_loss'], linewidth=2, marker='o', label='Train Loss')\n",
    "plt.plot(val_epochs, results['eval_loss'], linewidth=2, marker='o', label='Eval Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Poisson NLL Loss on Log(Counts)\")\n",
    "plt.title(r\"Log Regression: UNet Train & Val Loss, 10k, $\\lambda=0.001$\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26bd0153-0df9-427e-9b0d-f2f3ef370fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating from a Checkpoint\n",
    "# checkpoint = torch.load(f\"/sdf/home/c/carsmith/sdf_data/flash_detection_data/delay_200ks_ckpts/unet_2_2_10epochs.pth\", weights_only=True)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "# start_epoch = epochs + 1\n",
    "\n",
    "# more_epochs = 20\n",
    "# more_results = train_model_2(model, train_loader, val_loader, scheduler, optimizer, device, more_epochs, mode, logger)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "}, f\"/sdf/home/c/carsmith/sdf_data/flash_detection_data/delay_200ks_ckpts/unet_2_2_10epochs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e336fa2-27d4-4c46-8995-3c6e9e204a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'scheduler_state_dict': scheduler.state_dict(),\n",
    "# }, f\"/sdf/home/c/carsmith/sdf_data/flash_detection_data/class_reg_ckpts/norm_overtrain_unet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffe81d-505c-4742-9378-9cdab5495b2f",
   "metadata": {},
   "source": [
    "### Inspecting Classification Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab89fd3-0c18-4afb-9f1c-3406d1c30d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from a checkpoint\n",
    "# from model import *\n",
    "# from utils import *\n",
    "# model = UNet1D()\n",
    "# checkpoint = torch.load(\"class+reg_100.pth\", weights_only=True)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model = model.to('cuda')  # move model to GPU if needed\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e41d9-6ef8-4d96-82a4-c6f2bcad7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = 'cuda'\n",
    "train_progress = tqdm(val_loader, leave=False, position=0)\n",
    "for i, (data, target, hit_times, photon_target, photon_list) in enumerate(train_progress):\n",
    "    data, target, hit_times, photon_target = data.to(device), target.to(device), hit_times.to(device), photon_target.to(device) # model output is [25, 992] but target is [25, 1000] due to mismatch in downsampling/upsampling shapes\n",
    "    class_output, reg_output = model(data)\n",
    "    print(class_output.shape)\n",
    "    break\n",
    "\n",
    "epoch = 6\n",
    "loss, sampled_indices, masked_target, masked_output, output, target, masked_reg_output, masked_photon_target = mined_bce_loss(data, hit_times, photon_list, class_output, reg_output, epoch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ff328-51ed-4d34-a3b6-08011c6bbc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing validation targets\n",
    "# Inspecting Sampled Bins\n",
    "waveform_id = 2\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(masked_target[waveform_id*608:waveform_id*(608)+608].cpu(), marker='o', label='target')\n",
    "plt.plot(torch.sigmoid(masked_output[waveform_id*608:waveform_id*(608)+608]).detach().cpu(), marker='x', alpha=0.8, label='pred w/ sigmoid')\n",
    "\n",
    "hit_idx = hit_times[3].int()\n",
    "mask = sampled_indices[3].cpu()\n",
    "sampled_locs = torch.nonzero(mask).squeeze()\n",
    "true_hit_locs_in_masked = [i for i, idx in enumerate(sampled_locs) if idx.item() in hit_idx.tolist()]\n",
    "\n",
    "print(\"Hit times:\", hit_idx.tolist())\n",
    "print(\"Sampled indices:\", sampled_locs.tolist())\n",
    "print(\"True hit indices within masked:\", true_hit_locs_in_masked)\n",
    "\n",
    "indices = torch.nonzero(sampled_indices.cpu()[3] == True).squeeze().tolist()\n",
    "plt.xticks(ticks=range(len(indices)), labels=indices)\n",
    "\n",
    "plt.xlabel('sampled bins from one time window')\n",
    "plt.ylabel('value')\n",
    "plt.axhline(0.5, linestyle='--', color='r', label='reg bin cutoff')\n",
    "plt.title('Sampled Bins For 1 Time Window')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082103c7-9d18-4b4f-a401-84446b8b1dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(photon_target[waveform_id].sum())\n",
    "print(photon_list[waveform_id + 4])\n",
    "print(masked_photon_target[waveform_id * 20: waveform_id * 20 + 20].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a4c478-17ec-4ea4-bd31-d0a0f1b8b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing validation targets\n",
    "# Inspecting Sampled Bins\n",
    "waveform_id = 3\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(masked_photon_target[waveform_id * 20: waveform_id * 20 + 20].cpu(), marker='o', label='target')\n",
    "plt.plot(masked_reg_output[waveform_id * 20: waveform_id * 20 + 20].detach().cpu(), marker='o', label='pred')\n",
    "# plt.plot(torch.sigmoid(masked_reg_output[waveform_id*217:waveform_id*(217)+217]).detach().cpu(), marker='o', label='pred w/ sigmoid')\n",
    "\n",
    "plt.xticks(np.arange(20))\n",
    "plt.xlabel('sampled bin count')\n",
    "plt.ylabel('Number of Photons')\n",
    "# plt.axhline(0.5, linestyle='--', color='r', label='reg bin cutoff')\n",
    "plt.title('Sampled Bins For 1 Time Window (Classified as Positive)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1ab57-c7aa-4823-ba8f-8dcb4d08a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms, arrival_times, hit_times, photon_bins, photon_list = next(iter(train_loader))\n",
    "loss, sampled_indices, masked_target, masked_output, output, target, masked_reg_output, masked_photon_target = mined_bce_loss(data, hit_times, photon_list, class_output, reg_output, epoch, device)\n",
    "data, target, hit_times, photon_target, photon_list\n",
    "model.eval()\n",
    "class_output, reg_output = model(waveforms.to(device))\n",
    "\n",
    "print(f\"hit_times: {hit_times.shape}\")\n",
    "print(waveforms.shape)\n",
    "print(arrival_times.shape)\n",
    "ticks = np.arange(waveforms[0].shape[1])\n",
    "\n",
    "waveform_id = 2\n",
    "wf = waveforms[waveform_id]\n",
    "\n",
    "ticks = torch.arange(wf.shape[-1])  # assume ticks = [0, 1, ..., length-1]\n",
    "wf = wf.squeeze(0)  # shape: [length]\n",
    "\n",
    "plt.plot(ticks, wf, alpha=1, label='Waveform')\n",
    "\n",
    "# FOR MULTIPLE HITS!\n",
    "for j, t in enumerate(hit_times[waveform_id]):\n",
    "    if t < 0:\n",
    "        pass\n",
    "    else:\n",
    "        plt.axvline(\n",
    "            x=t.item(), \n",
    "            color='r', \n",
    "            linestyle='--', \n",
    "            linewidth=1, \n",
    "            label='True arrival' if j == 0 else \"\"\n",
    "        )\n",
    "\n",
    "class_output = class_output.squeeze(1)\n",
    "print(class_output.shape)\n",
    "mask = torch.sigmoid(class_output[waveform_id, :]) > 0.5   # shape [hits_per_batch]\n",
    "print(f\"mask shape {mask.shape}\")\n",
    "\n",
    "pred_hits = torch.nonzero(mask, as_tuple=False).squeeze(1) # shape [num_preds]\n",
    "print(pred_hits.shape)\n",
    "for j, t in enumerate(pred_hits):\n",
    "    plt.axvline(\n",
    "        x=t.item(),\n",
    "        color='g',\n",
    "        linestyle='--',\n",
    "        linewidth=1,\n",
    "        label='Pred arrival' if j == 0 else \"\"\n",
    "    )\n",
    "\n",
    "plt.title(f\"Full Waveform\")\n",
    "plt.xlabel(f\"Time Tick, 1 tick = 0.001 us\")\n",
    "plt.ylabel(\"ADC\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a5b32-a3ce-4b00-9440-de99ad621984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e8692d-7276-47bd-b4a1-4fe359b08459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import *\n",
    "\n",
    "model.eval()\n",
    "\n",
    "epochs = 1\n",
    "epoch = 0\n",
    "\n",
    "acc_progress = tqdm(val_loader, desc=f\"Scanning {epoch+1}/{epochs}\", leave=False, position=0)\n",
    "waveforms, arrival_times, hit_times, photon_bins, photon_list = next(iter(val_loader))\n",
    "\n",
    "val_acc = 0.0\n",
    "val_pure = 0.0\n",
    "# val_reg_acc = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target, hit_times, photon_target, photon_list) in enumerate(acc_progress):\n",
    "        data, target, photon_target = data.to(device), target.to(device), photon_target.to(device) # both [B, 1, 16000]\n",
    "        class_output, reg_output = model(data, mode='bce')\n",
    "\n",
    "        batch_acc = overall_class_acc(hit_times, class_output, device)\n",
    "        val_acc += batch_acc\n",
    "\n",
    "        batch_pure = overall_class_purity(hit_times, class_output, device)\n",
    "        val_pure += batch_pure\n",
    "\n",
    "        # reg_acc = regression_acc(photon_bins, reg_output, class_output, device)\n",
    "        # val_reg_acc += reg_acc\n",
    "\n",
    "val_acc = val_acc / len(val_loader)\n",
    "val_pure = val_pure / len(val_loader)\n",
    "# val_reg_acc = val_reg_acc / len(val_loader)\n",
    "\n",
    "print(f\"Overall Classification Accuracy: {val_acc}\")    \n",
    "print(f\"Overall Classification Purity: {val_pure}\")    \n",
    "# print(f\"Overall Regression Accuracy (Within 0.5): {val_reg_acc}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89384d-8b89-452b-858c-1f1c80495364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Class Accuracy\n",
    "waveforms, arrival_times, hit_times, photon_bins, photon_list = next(iter(val_loader))\n",
    "sample_id = 6\n",
    "waveforms = waveforms.to(device)\n",
    "class_output, reg_output = model(waveforms, mode='bce')\n",
    "class_output_cp = class_output.clone().squeeze(1)\n",
    "reg_output_cp = reg_output.clone().squeeze(1)\n",
    "photon_bins_cp = photon_bins.clone().squeeze(1)\n",
    "\n",
    "# predicted hits for this sample (indices)\n",
    "mask = torch.sigmoid(class_output_cp) > 0.5  # [B, L]\n",
    "pred_hits = torch.nonzero(mask[sample_id, :], as_tuple=False).squeeze(1).tolist()\n",
    "\n",
    "# true hits for this sample (assuming you have hit_times or photon_bins)\n",
    "true_hits = torch.nonzero(photon_bins_cp[sample_id, :], as_tuple=False).squeeze(1).tolist()\n",
    "print(true_hits)\n",
    "\n",
    "L = class_output_cp.shape[-1]  # number of bins\n",
    "\n",
    "# make indicator arrays\n",
    "pred_indicator = torch.zeros(L)\n",
    "true_indicator = torch.zeros(L)\n",
    "\n",
    "pred_indicator[pred_hits] = 1\n",
    "true_indicator[true_hits] = 1\n",
    "\n",
    "# convert to numpy for plotting\n",
    "pred_indicator = pred_indicator.numpy()\n",
    "true_indicator = true_indicator.numpy()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# --- Left plot: indicators ---\n",
    "axs[0].plot(pred_indicator, label=\"Predicted hits\", marker=\"o\", color='crimson')\n",
    "axs[0].plot(true_indicator, label=\"True hits\", marker=\"x\", color='mediumslateblue')\n",
    "axs[0].set_xlabel(\"Bin index\")\n",
    "axs[0].set_ylabel(\"Hit indicator\")\n",
    "axs[0].set_title(\"True Hits vs. Classified Hits\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# --- Right plot: regression output at predicted hits ---\n",
    "ticks = np.arange(len(pred_hits))\n",
    "abs_reg_output = torch.exp(reg_output_cp)\n",
    "axs[1].plot(ticks, abs_reg_output[sample_id, pred_hits].detach().cpu(), marker='o', label=\"Regression\", color='crimson')\n",
    "# axs[1].plot(ticks, reg_output_cp[sample_id, pred_hits].detach().cpu(), marker='o', label=\"Regression\", color='crimson')\n",
    "axs[1].plot(ticks, photon_bins_cp[sample_id, pred_hits], marker='o', label=\"Truth\", color='mediumslateblue')\n",
    "axs[1].set_xlabel(\"Classified Hit Index (subset)\")\n",
    "axs[1].set_ylabel(\"Photons\")\n",
    "axs[1].set_title(\"Truth vs. Pred Photons in Positively Classified Hit Bins\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(which='both', linestyle='--', linewidth=0.5)\n",
    "axs[1].minorticks_on()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f772e7-634f-42fc-8d9e-3a550a8d4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hybrid_loss import *\n",
    "# from model import *\n",
    "# from utils import *\n",
    "# model = UNet1D()\n",
    "# checkpoint = torch.load(f\"/sdf/home/c/carsmith/sdf_data/flash_detection_data/delay_200ks_ckpts/unet_2_2_10epochs.pth\", weights_only=True)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model = model.to('cuda')  # move model to GPU if needed\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ffef5-93ef-4154-a865-c8f6a4df4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying merging\n",
    "# Visualizing Class Accuracy\n",
    "device = 'cuda'\n",
    "waveforms, arrival_times, hit_times, photon_bins, photon_list = next(iter(test_loader))\n",
    "sample_id = 2\n",
    "waveforms = waveforms.to(device)\n",
    "class_output, reg_output = model(waveforms, mode='bce')\n",
    "class_output_cp = class_output.clone().squeeze(1)\n",
    "reg_output_cp = reg_output.clone().squeeze(1)\n",
    "photon_bins_cp = photon_bins.clone().squeeze(1)\n",
    "loss, sampled_indices, masked_target, masked_output, output, target, masked_reg_output, masked_photon_target = mined_bce_loss(waveforms, hit_times, photon_list, class_output, reg_output, epoch, device)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Plot 1: Full waveform with true + predicted hits\n",
    "# ------------------------------\n",
    "wf = waveforms[sample_id]\n",
    "ticks = torch.arange(wf.shape[-1])  # assume ticks = [0, 1, ..., length-1]\n",
    "wf = wf.squeeze(0)  # shape: [length]\n",
    "axes[0].plot(ticks, wf.cpu(), alpha=1, label='Waveform')\n",
    "\n",
    "# True hits\n",
    "for j, t in enumerate(hit_times[sample_id]):\n",
    "    if t < 0:\n",
    "        pass\n",
    "    else:\n",
    "        axes[0].axvline(\n",
    "            x=int(t.item()), \n",
    "            color='r', \n",
    "            linestyle='--', \n",
    "            linewidth=2, \n",
    "            label='True arrival' if j == 0 else \"\"\n",
    "        )\n",
    "\n",
    "# Predicted hits\n",
    "class_output = class_output.squeeze(1)\n",
    "mask = torch.sigmoid(class_output[sample_id, :]) > 0.5\n",
    "pred_hits = torch.nonzero(mask, as_tuple=False).squeeze(1)\n",
    "for j, t in enumerate(pred_hits):\n",
    "    axes[0].axvline(\n",
    "        x=t.item(),\n",
    "        color='g',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        label='Pred arrival' if j == 0 else \"\"\n",
    "    )\n",
    "    \n",
    "axes[0].set_title(\"Full Waveform\")\n",
    "axes[0].set_ylabel(\"ADC\", fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].tick_params(axis='y', labelsize=12)\n",
    "axes[0].grid(True)\n",
    "\n",
    "# ------------------------------\n",
    "# Plot 2: Sampled bins for Classification Loss\n",
    "# ------------------------------\n",
    "num_samples_per_wf = sampled_indices.sum(dim=1)\n",
    "cum_samples = torch.cumsum(num_samples_per_wf, dim=0)   # [batch_size]\n",
    "starts = torch.cat([torch.tensor([0], device=cum_samples.device), cum_samples[:-1]])\n",
    "start = starts[sample_id].item()\n",
    "end   = cum_samples[sample_id].item()\n",
    "\n",
    "sampled_locs = torch.nonzero(sampled_indices[sample_id].cpu()).squeeze().tolist()\n",
    "idx_tensor   = torch.tensor(sampled_locs, dtype=torch.long)\n",
    "\n",
    "y_target = masked_target[start:end].cpu()\n",
    "y_pred   = torch.sigmoid(masked_output[start:end]).cpu().detach()\n",
    "\n",
    "axes[1].scatter(idx_tensor, y_target, marker='o', label='target')\n",
    "axes[1].scatter(idx_tensor, y_pred, marker='x', alpha=1.0, label='pred w/ sigmoid')\n",
    "\n",
    "merged_mask = merge_bins(class_output.unsqueeze(1), skip_tol=1)  # [B, 1, L]\n",
    "mask_row = merged_mask[sample_id, 0]  # [L]\n",
    "intervals = mask_to_intervals(mask_row)\n",
    "\n",
    "# intervals is now like [(10, 35), (50, 62), ...]\n",
    "# Plot shading\n",
    "for j, (s_bin, e_bin) in enumerate(intervals):\n",
    "    axes[1].axvspan(\n",
    "        s_bin, e_bin,\n",
    "        facecolor='mediumseagreen', alpha=0.8, zorder=-1,\n",
    "        label=\"Merged Hit Window\" if j == 0 else None\n",
    "    )\n",
    "\n",
    "axes[1].set_ylabel('Sigmoid Value', fontsize=12)\n",
    "axes[1].axhline(0.5, linestyle='--', color='r', label='reg bin cutoff')\n",
    "axes[1].set_title('Classification Loss: Sampled Bins')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(axes[0].get_xlim())\n",
    "axes[1].tick_params(axis='y', labelsize=12)\n",
    "axes[1].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2adfd-697b-487c-9198-5e4d9f210886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_intervals(mask_row):\n",
    "    \"\"\"\n",
    "    Convert a 1D mask (shape [L]) of 0/1 values into list of (start, end) intervals.\n",
    "    \"\"\"\n",
    "    mask_row = mask_row.cpu().numpy()\n",
    "    intervals = []\n",
    "    in_region = False\n",
    "    for i, v in enumerate(mask_row):\n",
    "        if v == 1 and not in_region:\n",
    "            start = i\n",
    "            in_region = True\n",
    "        elif v == 0 and in_region:\n",
    "            end = i - 1\n",
    "            intervals.append((start, end))\n",
    "            in_region = False\n",
    "    if in_region:\n",
    "        intervals.append((start, len(mask_row) - 1))\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f2a67-c8ca-42ba-b391-b4ee4c5f9561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a true hit (e.g., the first valid one for this sample)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "true_hits = [t.item() for t in hit_times[sample_id] if t.item() >= 0]\n",
    "if len(true_hits) == 0:\n",
    "    raise ValueError(\"No valid hits for this sample\")\n",
    "center_bin = int(true_hits[0])  # take first hit, or pick another\n",
    "\n",
    "# Define zoom window\n",
    "half_window = 100\n",
    "zoom_start = max(0, center_bin - half_window)\n",
    "zoom_end   = min(class_output.shape[-1], center_bin + half_window)\n",
    "\n",
    "# ------------------------------\n",
    "# Plot 1: Zoomed waveform\n",
    "# ------------------------------\n",
    "wf = waveforms[sample_id].squeeze(0)  # shape [length]\n",
    "ticks = torch.arange(wf.shape[-1])\n",
    "\n",
    "axes[0].plot(ticks[zoom_start:zoom_end], wf[zoom_start:zoom_end].cpu(), alpha=1, label='Waveform')\n",
    "\n",
    "# True hits in zoom\n",
    "for j, t in enumerate(true_hits):\n",
    "    if zoom_start <= t < zoom_end:\n",
    "        axes[0].axvline(\n",
    "            x=t, color='r', linestyle='--', linewidth=2,\n",
    "            label='True arrival' if j == 0 else \"\"\n",
    "        )\n",
    "\n",
    "# Predicted hits in zoom\n",
    "mask = torch.sigmoid(class_output[sample_id, :]) > 0.5\n",
    "pred_hits = torch.nonzero(mask, as_tuple=False).squeeze(1)\n",
    "for j, t in enumerate(pred_hits):\n",
    "    t = t.item()\n",
    "    if zoom_start <= t < zoom_end:\n",
    "        axes[0].axvline(\n",
    "            x=t, color='g', linestyle='--', linewidth=2,\n",
    "            label='Pred arrival' if j == 0 else \"\"\n",
    "        )\n",
    "\n",
    "axes[0].set_title(f\"Waveform Zoom (bins {zoom_start}–{zoom_end})\")\n",
    "axes[0].set_ylabel(\"ADC\", fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].tick_params(axis='y', labelsize=12)\n",
    "axes[0].grid(True)\n",
    "axes[0].set_xlim([zoom_start, zoom_end])\n",
    "\n",
    "# ------------------------------\n",
    "# Plot 2: Classification Loss in zoom\n",
    "# ------------------------------\n",
    "sampled_locs = torch.nonzero(sampled_indices[sample_id].cpu()).squeeze().tolist()\n",
    "idx_tensor   = torch.tensor(sampled_locs, dtype=torch.long)\n",
    "\n",
    "y_target = masked_target[start:end].cpu()\n",
    "y_pred   = torch.sigmoid(masked_output[start:end]).cpu().detach()\n",
    "\n",
    "# Only keep bins within zoom window\n",
    "mask_zoom = (idx_tensor >= zoom_start) & (idx_tensor < zoom_end)\n",
    "idx_tensor_zoom = idx_tensor[mask_zoom]\n",
    "y_target_zoom = y_target[mask_zoom]\n",
    "y_pred_zoom   = y_pred[mask_zoom]\n",
    "\n",
    "axes[1].scatter(idx_tensor_zoom, y_target_zoom, marker='o', label='target')\n",
    "axes[1].scatter(idx_tensor_zoom, y_pred_zoom, marker='x', alpha=1.0, label='pred w/ sigmoid')\n",
    "\n",
    "# Merged intervals (mask already computed)\n",
    "mask_row = merged_mask[sample_id, 0]\n",
    "intervals = mask_to_intervals(mask_row)\n",
    "\n",
    "for j, (s_bin, e_bin) in enumerate(intervals):\n",
    "    if e_bin < zoom_start or s_bin > zoom_end:\n",
    "        continue\n",
    "    axes[1].axvspan(\n",
    "        max(s_bin, zoom_start), min(e_bin, zoom_end),\n",
    "        facecolor='mediumseagreen', alpha=0.8, zorder=-1,\n",
    "        label=\"Merged Hit Window\" if j == 0 else None\n",
    "    )\n",
    "\n",
    "axes[1].set_ylabel('Sigmoid Value', fontsize=12)\n",
    "axes[1].axhline(0.5, linestyle='--', color='r', label='reg bin cutoff')\n",
    "axes[1].set_title('Classification Loss: Zoomed Bins')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim([zoom_start, zoom_end])\n",
    "axes[1].tick_params(axis='y', labelsize=12)\n",
    "axes[1].grid(True)\n",
    "\n",
    "true_hits = [t.item() for t in hit_times[sample_id] if t.item() >= 0]\n",
    "if len(true_hits) == 0:\n",
    "    raise ValueError(\"No valid hits for this sample\")\n",
    "center_bin = int(true_hits[0])  # take first hit, or pick another\n",
    "\n",
    "# Define zoom window\n",
    "half_window = 100\n",
    "zoom_start = max(0, center_bin - half_window)\n",
    "zoom_end   = min(class_output.shape[-1], center_bin + half_window)\n",
    "\n",
    "# ------------------------------\n",
    "# Plot 1: Zoomed waveform\n",
    "# ------------------------------\n",
    "wf = waveforms[sample_id].squeeze(0)  # shape [length]\n",
    "ticks = torch.arange(wf.shape[-1])\n",
    "\n",
    "axes[0].plot(ticks[zoom_start:zoom_end], wf[zoom_start:zoom_end].cpu(), alpha=1, label='Waveform')\n",
    "\n",
    "# True hits in zoom\n",
    "for j, t in enumerate(true_hits):\n",
    "    if zoom_start <= t < zoom_end:\n",
    "        axes[0].axvline(\n",
    "            x=t, color='r', linestyle='--', linewidth=2,\n",
    "            label='True arrival' if j == 0 else \"\"\n",
    "        )\n",
    "\n",
    "# Predicted hits in zoom\n",
    "mask = torch.sigmoid(class_output[sample_id, :]) > 0.5\n",
    "pred_hits = torch.nonzero(mask, as_tuple=False).squeeze(1)\n",
    "for j, t in enumerate(pred_hits):\n",
    "    t = t.item()\n",
    "    if zoom_start <= t < zoom_end:\n",
    "        axes[0].axvline(\n",
    "            x=t, color='g', linestyle='--', linewidth=2,\n",
    "            label='Pred arrival' if j == 0 else \"\"\n",
    "        )\n",
    "\n",
    "axes[0].set_title(f\"Waveform Zoom (bins {zoom_start}–{zoom_end})\")\n",
    "axes[0].set_ylabel(\"ADC\", fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].tick_params(axis='y', labelsize=12)\n",
    "axes[0].grid(True)\n",
    "axes[0].set_xlim([zoom_start, zoom_end])\n",
    "\n",
    "# ------------------------------\n",
    "# Plot 2: Classification Loss in zoom\n",
    "# ------------------------------\n",
    "sampled_locs = torch.nonzero(sampled_indices[sample_id].cpu()).squeeze().tolist()\n",
    "idx_tensor   = torch.tensor(sampled_locs, dtype=torch.long)\n",
    "\n",
    "y_target = masked_target[start:end].cpu()\n",
    "y_pred   = torch.sigmoid(masked_output[start:end]).cpu().detach()\n",
    "\n",
    "# Only keep bins within zoom window\n",
    "mask_zoom = (idx_tensor >= zoom_start) & (idx_tensor < zoom_end)\n",
    "idx_tensor_zoom = idx_tensor[mask_zoom]\n",
    "y_target_zoom = y_target[mask_zoom]\n",
    "y_pred_zoom   = y_pred[mask_zoom]\n",
    "\n",
    "axes[1].scatter(idx_tensor_zoom, y_target_zoom, marker='o', label='target')\n",
    "axes[1].scatter(idx_tensor_zoom, y_pred_zoom, marker='x', alpha=1.0, label='pred w/ sigmoid')\n",
    "\n",
    "# Merged intervals (mask already computed)\n",
    "mask_row = merged_mask[sample_id, 0]\n",
    "intervals = mask_to_intervals(mask_row)\n",
    "\n",
    "for j, (s_bin, e_bin) in enumerate(intervals):\n",
    "    if e_bin < zoom_start or s_bin > zoom_end:\n",
    "        continue\n",
    "    axes[1].axvspan(\n",
    "        max(s_bin, zoom_start), min(e_bin, zoom_end),\n",
    "        facecolor='mediumseagreen', alpha=0.8, zorder=-1,\n",
    "        label=\"Merged Hit Window\" if j == 0 else None\n",
    "    )\n",
    "\n",
    "axes[1].set_ylabel('Sigmoid Value', fontsize=12)\n",
    "axes[1].axhline(0.5, linestyle='--', color='r', label='reg bin cutoff')\n",
    "axes[1].set_title('Classification Loss: Zoomed Bins')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim([zoom_start, zoom_end])\n",
    "axes[1].tick_params(axis='y', labelsize=12)\n",
    "axes[1].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb461066-af95-4911-8911-0be856006ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18b24c-1ac7-4aff-ac1d-ffb3d714c23a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
